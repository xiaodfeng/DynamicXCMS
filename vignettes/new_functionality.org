#+TITLE: New and modified functionality in xcms
#+AUTHOR:    Johannes Rainer
#+EMAIL:     johannes.rainer@eurac.edu
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS: ^:{} toc:nil
#+PROPERTY: header-args :exports code
#+PROPERTY: header-args :session *R*

#+BEGIN_EXPORT html
---
title: "New and modified functionality in xcms"
author:
- name: Johannes Rainer
package: xcms
output:
  BiocStyle::html_document:
    toc_float: true
vignette: >
  %\VignetteIndexEntry{New and modified functionality in xcms}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{xcms,RColorBrewer}
bibliography: references.bib
csl: biomed-central.csl
references:
- id: dummy
  title: no title
  author:
  - family: noname
    given: noname
---

#+END_EXPORT

#+NAME: biocstyle
#+BEGIN_SRC R :ravel echo = FALSE, results = "asis"
  BiocStyle::markdown()
#+END_SRC

* New functionality in =xcms=

This document describes new functionality and changes to existing functionality
in the =xcms= package introduced during the update to version /3/.

#+BEGIN_SRC R :ravel message = FALSE, warning = FALSE
  library(xcms)
  library(RColorBrewer)
  register(SerialParam())
#+END_SRC

** Modernized user interface

The modernization of the user interface comprises new classes for data
representation and new data analysis methods. In addition, the core logic for
the data processing has been extracted from the old methods and put into a set
of R functions, the so called core API functions (or =do_= functions). These
functions take standard R data structures as input and return standard R data
types as result and can hence be easily included in other R packages.

The new user interface aims at simplifying and streamlining the =xcms= workflow
while guaranteeing data integrity and performance also for large scale
metabolomics experiments. Importantly, a simplified access to the original raw
data should be provided throughout the whole metabolomics data analysis workflow.

# All objects in the new user interface ensuring
# data integrity /via/ validation methods and class versioning, all methods are
# tested internally in extensive unit tests to guarantee proper functionality.

The new interface re-uses objects from the =MSnbase= Bioconductor package, such as
the =OnDiskMSnExp= object. This object is specifically designed for large scale MS
experiments as it initially reads just the scan header information from the mzML
while the mz-intensity value pairs from all or from selected spectra of a file
are read on demand hence minimizing the memory demand. Also, in contrast to
the old =xcmsRaw= object, the =OnDiskMSnExp= contains information from all files of
an experiment. In addition, all data normalization and adjustment methods
implemented in the =MSnbase= package can be directly applied to the MS data
without the need to re-implement such methods in =xcms=. Results from =xcms=
preprocessings, such as chromatographic peak detection or correspondence are
stored into the new =XCMSnExp= object. This object extends the =OnDiskMSnExp= object
and inherits thus all of its methods including raw data access.

Class and method/function names follow also a new naming convention trying tp
avoid the partially confusing nomenclature of the original =xcms= methods (such as
the =group= method to perform the correspondence of peaks across samples). To
distinguish them from mass peaks, the peaks identified by the peak detection in
an LS/GC-MS experiment are referred to as /chromatographic peaks/. The respective
method to identify such peaks is hence called =findChromPeaks= and the identified
peaks can be accessed using the =XCMSnExp= =chromPeaks= method. The results from an
correspondence analysis which aims to match and group chromatographic peaks
within and between samples are called /features/. A feature corresponds to
individual ions with a unique mass-to-charge ratio (mz) and a unique retention
time (rt). The definition of such mz-rt features (i.e. the result from the
=groupChromPeaks= method) can be accessed /via/ the =featureDefinitions= method of
the =XCMSnExp= class. Finally, alignment (retention time correction) can be
performed using the =adjustRtime= method.

The settings for any of the new analysis methods are bundled in /parameter/
classes, one class for each method. This encapsulation of the parameters to a
function into a parameter class (such as =CentWaveParam=) avoids busy function
calls (with many single parameters) and enables saving, reloading and reusing
the settings. In addition, the parameter classes are added, along with other
information to the process history of an =XCMSnExp= object thus providing a
detailed documentation of each processing step of an analysis, with the
possibility to recall all settings of the performed analyses at any stage. In
addition, validation of the parameters can be performed within the parameter
object and hence is no longer required in the analysis function.

** New naming convention

Peaks identified in LC/GC-MS metabolomics are referred to as /chromatographic
peaks/ where possible to avoid any misconceptions with /mass peaks/ identified in
mz dimension.

Methods for data analysis from the original =xcms= code have been renamed to avoid
potential confusions:

+ *Chromatographic peak detection*: =findChromPeaks= instead of =findPeaks=: for new
  functions and methods the term /peak/ is avoided as much as possible, as it is
  usually used to describe a mass peak in mz dimension. To clearly distinguish
  between these peaks and peaks in retention time space, the latter are referred
  to as /chromatographic peak/, or =chromPeak=.

+ *Correspondence*: =groupChromPeaks= instead of =group= to clearly indicate what is
  being grouped. Group might be a sample group or a peak group, the latter being
  referred to also by (mz-rt) /feature/.
  
+ *Alignment*: =adjustRtime= instead of =retcor= for retention time correction. The
  word /cor/ in /retcor/ might be easily misinterpreted as /correlation/ instead of
  correction.

** New data classes

*** =OnDiskMSnExp=

This object is defined and documented in the =MSnbase= package. In brief, it is a
container for the full raw data from an MS-based experiment. To keep the memory
footprint low the mz and intensity values are only loaded from the raw data
files when required. The =OnDiskMSnExp= object replaces the =xcmsRaw= object.

*** =XCMSnExp=

The =XCMSnExp= class extends the =OnDiskMSnExp= object from the =MSnbase= package and
represents a container for the xcms-based preprocessing results while (since it
inherits all functionality from its parent class) keeping a direct relation to
the (raw) data on which the processing was performed. An additional slot
=.processHistory= in the object allows to keep track of all performed processing
steps. Each analysis method, such as =findChromPeaks= adds an =XProcessHistory=
object which includes also the parameter class passed to the analysis
method. Hence not only the time and type of the analysis, but its exact settings
are reported within the =XCMSnExp= object. The =XCMSnExp= is thus equivalent to the
=xcmsSet= from the original =xcms= implementation, but keeps in addition a link to
the raw data on which the preprocessing was performed.

*** =Chromatogram=

The =Chromatogram= class (available in the =MSnbase= package since version 2.3.8)
allows a data representation that is orthogonal to the =Spectrum= class (also
defined in =MSnbase=). The =Chromatogram= class stores retention time and intensity
duplets and is designed to accommodate most use cases, from total ion
chromatogram, base peak chromatogram to extracted ion chromatogram and SRM/MRM
ion traces.

=Chromatogram= objects can be extracted from =XCMSnExp= (and =MSnExp= and
=OnDiskMSnExp=) objects using the =chromatogram= method.

Note that this class is still considered developmental and might thus undergo
some changes in the future.

** Binning and missing value imputation functions

The binning/profile matrix generation functions have been completely
rewritten. The new =binYonX= function replaces the binning of intensity values
into bins defined by their m/z values implemented in the =profBin=, =profBinLin= and
=profBinLinBase= methods. The =binYonX= function provides also additional functionality:

+ Breaks for the bins can be defined based on either the number of desired bins
  (=nBins=) or the size of a bin (=binSize=). In addition it is possible to provide
  a vector with pre-defined breaks. This allows to bin data from multiple files
  or scans on the same bin-definition.

+ The function returns a list with element =y= containing the binned values and
  element =x= the bin mid-points.

+ Values in input vector =y= can be aggregated within each bin with different
  methods: =max=, =min=, =sum= and =mean=.

+ The index of the largest (or smallest for =method= being "min") within each bin
  can be returned by setting argument =returnIndex= to =TRUE=.

+ Binning can be performed on single or multiple sub-sets of the input vectors
  using the =fromIdx= and =toIdx= arguments. This replaces the /M/ methods (such as
  =profBinM=). These sub-sets can be overlapping.

The missing value imputation logic inherently build into the =profBinLin= and
=profBinLinBase= methods has been implemented in the =imputeLinInterpol= function.

The example below illustrates the binning and imputation with the =binYtoX= and
=imputeLinInterpol= functions. After binning of the test vectors below some of the
bins have missing values, for which we impute a value using
=imputeLinInterpol=. By default, =binYonX= selects the largest value within each
bin, but other aggregation methods are also available (i.e. min, max, mean,
sum).

#+BEGIN_SRC R :ravel message = FALSE
  ## Defining the variables:
  set.seed(123)
  X <- sort(abs(rnorm(30, mean = 20, sd = 25))) ## 10
  Y <- abs(rnorm(30, mean = 50, sd = 30))

  ## Bin the values in Y into 20 bins defined on X
  res <- binYonX(X, Y, nBins = 22)

  res
#+END_SRC

As a result we get a =list= with the bin mid-points (=$x=) and the binned =y= values
(=$y=).

Next we use two different imputation approaches, a simple linear interpolation
and the linear imputation approach that was defined in the =profBinLinBase=
method. The latter performs linear interpolation only considering a certain
neighborhood of missing values otherwise replacing the =NA= with a base value.

#+BEGIN_SRC R :ravel binning-imputation-example, message = FALSE, fig.width = 10, fig.height = 7, fig.cap = 'Binning and missing value imputation results. Black points represent the input values, red the results from the binning and blue and green the results from the imputation (with method lin and linbase, respectively).'
  ## Plot the actual data values.
  plot(X, Y, pch = 16, ylim = c(0, max(Y)))
  ## Visualizing the bins
  abline(v = breaks_on_nBins(min(X), max(X), nBins = 22), col = "grey")

  ## Define colors:
  point_colors <- paste0(brewer.pal(4, "Set1"), 80)
  ## Plot the binned values.
  points(x = res$x, y = res$y, col = point_colors[1], pch = 15)

  ## Perform the linear imputation.
  res_lin <- imputeLinInterpol(res$y)

  points(x = res$x, y = res_lin, col = point_colors[2], type = "b")

  ## Perform the linear imputation "linbase"
  res_linbase <- imputeLinInterpol(res$y, method = "linbase")
  points(x = res$x, y = res_linbase, col = point_colors[3], type = "b", lty = 2)
#+END_SRC

The difference between the linear interpolation method =lin= and =linbase= is that
the latter only performs the linear interpolation in a pre-defined neighborhood
of the bin with the missing value (=1= by default). The other missing values are
set to a base value corresponding to half of the smallest bin value. Both
methods thus yield same results, except for bins 15-17 (see Figure above).

** Core functionality exposed /via/ simple functions

The core logic from the chromatographic peak detection methods
=findPeaks.centWave=, =findPeaks.massifquant=, =findPeaks.matchedFilter= and
=findPeaks.MSW= and from all alignment (=group.*=) and correspondence (=retcor.*=)
methods has been extracted and put into functions with the common prefix
=do_findChromPeaks=, =do_adjustRtime= and =do_groupChromPeaks=, respectively, with the
aim, as detailed in issue [[https://github.com/sneumann/xcms/issues/30][#30]], to separate the core logic from the analysis
methods invoked by the users to enable also the use these methods using base R
parameters (i.e. without specific classes containing the data such as the
=xcmsRaw= class). This simplifies also the re-use of these functions in other
packages and simplifies the future implementation of the peak detection
algorithms for e.g. the =MSnExp= or =OnDiskMSnExp= objects from the =MSnbase=
Bioconductor package. The implemented functions are:

+ *peak detection methods*:
  + =do_findChromPeaks_centWave=: peak density and wavelet based peak detection
    for high resolution LC/MS data in centroid mode \cite{Tautenhahn:2008fx}.
  + =do_findChromPeaks_matchedFilter=: identification of peak in the
    chromatographic domain based on matched filtration \cite{Smith:2006ic}.
  + =do_findChromPeaks_massifquant=: identification of peaks using Kalman
    filters.
  + =do_findChromPeaks_MSW=: single spectrum, non-chromatographic peak detection.

+ *alignment methods*:
  + =do_adjustRtime_peakGroups=: perform sample alignment (retention time
    correction) using alignment of /well behaved/ chromatographic peaks that are
    present in most samples (and are expected to have the same retention time).

+ *correspondence methods*:
  + =do_groupChromPeaks_density=: perform chromatographic peak grouping (within
    and across samples) based on the density distribution of peaks along the
    retention time axis.
  + =do_groupChromPeaks_nearest=: groups peaks across samples similar to the
    method implemented in mzMine.
  + =do_groupChromPeaks_mzClust=: performs high resolution correspondence on
    single spectra samples.

One possible drawback from the introduction of this new layer is, that more
objects get copied by R which /could/ eventually result in a larger memory demand
or performance decrease (while no such was decrease was observed up to now).

** Usability improvements in the /old/ user interface

+ =[= subsetting method for =xcmsRaw= objects that enables to subset an =xcmsRaw=
  object to specific scans/spectra.
+ =profMat= method to extract the /profile/ matrix from the =xcmsRaw= object. This
  method should be used instead of directly accessing the =@env$profile= slot, as
  it will create the profile matrix on the fly if it was not pre-calculated (or
  if profile matrix generation settings have been changed).

* Changes due to bug fixes and modified functionality

** Differences in linear interpolation of missing values (=profBinLin=).

From =xcms= version 1.51.1 on the new binning functions are used, thus, the bug
described here are fixed.

Two bugs are present in the =profBinLin= method (reported as issues [[https://github.com/sneumann/xcms/issues/46][#46]] and [[https://github.com/sneumann/xcms/issues/49][#49]] on
github) which are fixed in the new =binYonX= and =imputeLinInterpol= functions:

+ The first bin value calculated by =profBinLin= can be wrong (i.e. not being the
  max value within that bin, but the first).
+ If the last bin contains also missing values, the method fails to determine
  a correct value for that bin.

The =profBinLin= method is used in =findPeaks.matchedFilter= if the profile
method is set to "binlin".

The example below illustrates both differences.

#+BEGIN_SRC R
  ## Define a vector with empty values at the end.
  X <- 1:11
  set.seed(123)
  Y <- sort(rnorm(11, mean = 20, sd = 10))
  Y[9:11] <- NA
  nas <- is.na(Y)
  ## Do interpolation with profBinLin:
  resX <- xcms:::profBinLin(X[!nas], Y[!nas], 5, xstart = min(X),
                            xend = max(X))
  resX
  res <- binYonX(X, Y, nBins = 5L, shiftByHalfBinSize = TRUE)
  resM <- imputeLinInterpol(res$y, method = "lin",
                            noInterpolAtEnds = TRUE)
  resM
#+END_SRC

Plotting the results helps to better compare the differences. The black points
in the figure below represent the actual values of =Y= and the grey vertical lines
the breaks defining the bins. The blue lines and points represent the result
from the =profBinLin= method. The bin values for the first and 4th bin are clearly
wrong. The green colored points and lines represent the results from the =binYonX=
and =imputeLinInterpol= functions (showing the correct binning and interpolation).

#+BEGIN_SRC R :ravel profBinLin-problems, message = FALSE, fig.align = 'center', fig.width=10, fig.height = 7, fig.cap = "Illustration of the two bugs in profBinLin. The input values are represented by black points, grey vertical lines indicate the bins. The results from binning and interpolation with profBinLin are shown in blue and those from binYonX in combination with imputeLinInterpol in green."
  plot(x = X, y = Y, pch = 16, ylim = c(0, max(Y, na.rm = TRUE)),
       xlim = c(0, 12))
  ## Plot the breaks
  abline(v = breaks_on_nBins(min(X), max(X), 5L, TRUE), col = "grey")
  ## Result from profBinLin:
  points(x = res$x, y = resX, col = "blue", type = "b")
  ## Results from imputeLinInterpol
  points(x = res$x, y = resM, col = "green", type = "b",
         pch = 4, lty = 2)

#+END_SRC

Note that by default =imputeLinInterpol= would also interpolate missing values at
the beginning and the end of the provided numeric vector. This can be disabled
(to be compliant with =profBinLin=) by setting parameter =noInterpolAtEnds= to
=TRUE= (like in the example above).

** Differences due to updates in =do_findChromPeaks_matchedFilter=, respectively =findPeaks.matchedFilter=.

The original =findPeaks.matchedFilter= (up to version 1.49.7) had several
shortcomings and bugs that have been fixed in the new
=do_findChromPeaks_matchedFilter= method:

+ The internal iterative processing of smaller chunks of the full data (also
  referred to as /iterative buffering/) could result, for some bin (step) sizes to
  unstable binning results (discussed in issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github): calculation of
  the breaks, or to be precise, the actually used bin size was performed in each
  iteration and could lead to slightly different sizes between iterations (due
  to rounding errors caused by floating point number representations in C).

+ The iterative buffering raises also a conceptual issue when linear
  interpolation is performed to impute missing values: the linear imputation
  will only consider values within the actually processed buffer and can thus
  lead to wrong or inaccurate imputations.

+ The =profBinLin= implementation contains two bugs, one that can result in
  failing to identify the maximal value in the first and last bin (see issue
  [[https://github.com/sneumann/xcms/issues/46][#46]]) and one that fails to assign a value to a bin (issue [[https://github.com/sneumann/xcms/issues/49][#49]]). Both are fixed
  in the =do_findChromPeaks_matchedFilter= implementation.

A detailed description of tests comparing all implementations is available in
issue [[https://github.com/sneumann/xcms/issues/52][#52]] on github. Note also that in course of these changes also the =getEIC=
method has been updated to use the new binning and missing value imputation
function.

While it is strongly discouraged, it is still possible to use to /old/ code (from
1.49.7) by calling =useOriginalCode(TRUE)=.

** Differences in =findPeaks.massifquant=

+ Argument =scanrange= was ignored in the /original/ old code (issue [[https://github.com/sneumann/xcms/issues/61][#61]]).
+ The method returned a =matrix= if =withWave= was =0= and a =xcmsPeaks= object
  otherwise. The updated version returns *always* an =xcmsPeaks= object (issue #60).

** Differences in /obiwarp/ retention time correction

Retention time correction using the obiwarp method uses the /profile/ matrix
(i.e. intensities binned in discrete bins along the mz axis). Profile matrix
generation uses now the =binYonX= method which fixed some problems in the original
binning and linear interpolation methods. Thus results might be slightly
different.

Also, the =retcor.obiwarp= method reports (un-rounded) adjusted retention times,
but adjusts the retention time of eventually already identified peaks using
rounded adjusted retention times. The new =adjustRtime= method(s) does adjust
identified peaks using the reported adjusted retention times (not rounded). This
guarantees that e.g. removing retention time adjustment/alignment results from
an object restores the object to its initial state (i.e. the adjusted retention
times of the identified peaks are reverted to the retention times before
alignment).
See issue [[https://github.com/sneumann/xcms/issues/122][#122]] for more details.

** =retcor.peaksgroups=: change in the way how /well behaved/ peak groups are ordered

The =retcor.peakgroups= defines first the chromatographic peak groups that are
used for the alignment of all spectra. Once these are identified, the retention
time of the peak with the highest intensity in a sample for a given peak group
is returned and the peak groups are ordered increasingly by retention time
(which is required for the later fitting of either a polynomial or a linear
model to the data). The selection of the retention time of the peak with the
highest intensity within a feature (peak group) and samples, denoted as
/representative/ peak for a given feature in a sample, ensures that only the
retention time of a single peak per sample and feature is selected (note that
multiple chromatographic peaks within the same sample can be assigned to a
feature).  In the original code the ordering of the peak groups was however
performed using the median retention time of the complete peak group (which
includes also potential additional peaks per sample). This has been changed and
the features are ordered now by the median retention time across samples of the
representative chromatographic peaks.

** =scanrange= parameter in all =findPeaks= methods

The =scanrange= in the =findPeaks= methods is supposed to enable the peak detection
only within a user-defined range of scans. This was however not performed in
each method. Due to a bug in =findPeaks.matchedFilter='s original code the
argument was ignored, except if the upper scan number of the user defined range
was larger than the total number of available scans (see issue [[https://github.com/sneumann/xcms/issues/63][#63]]). In
=findPeaks.massifquant= the argument was completely ignored (see issue [[https://github.com/sneumann/xcms/issues/61][#61]]) and,
while the argument was considered in =findPeaks.centWave= and feature detection
was performed within the specified scan range, but the original =@scantime= slot
was used throughout the code instead of just the scan times for the specified
scan indices (see issue [[https://github.com/sneumann/xcms/issues/64][#64]]).

These problems have been fixed in version 1.51.1 by first sub-setting the
=xcmsRaw= object (using the =[= method) before actually performing the feature
detection.

** =fillPeaks= (=fillChromPeaks=) differences
   
In the original =fillPeaks.MSW=, the mz range from which the signal is to be
integrated was defined using 

#+BEGIN_SRC R :eval = "never", :ravel eval = FALSE
  mzarea <- seq(which.min(abs(mzs - peakArea[i, "mzmin"])),
		which.min(abs(mzs - peakArea[i, "mzmax"])))

#+END_SRC

Depending on the data this could lead to the inclusion of signal in the
integration that are just outside of the mz range. In the new =fillChromPeaks=
method signal is integrated only for mz values >= mzmin and <= mzmax thus
ensuring that only signal is used that is truly within the peak area defined by
columns ="mzmin"=, ="mzmax"=, ="rtmin"= and ="rtmax"=.

Also, the =fillPeaks.chrom= method did return ="into"= and ="maxo"= values of =0= if no
signal was found in the peak area. The new method does not integrate any signal
in such cases and does not fill in that peak.

See also issue [[https://github.com/sneumann/xcms/issues/130][#130]] for more
information.

** Problems with iterative binning of small data sub-sets in =findPeaks.matchedFilter= :noexport:

The problem described here has been fixed in =xcms= >= 1.51.1.

The iterative binning of only small sub-sets of data causes problems with
=profBinLinBase=, in which data imputation might be skipped in some iterations
while it is performed in others (also discussed in issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github).

Iterative buffering has both conceptual and computational issues.
+ Conceptual: =profBinLin= and =profBinLinBase= do a linear interpolation to impute
  missing values. This is obviously affected by the input data, i.e. if only a
  small subset of input data is considered, the imputation can change.

+ Computational: the iterative buffering is slower than binning of the full
  data.

An additional problem comes with the implementation of the =profBin= method in
=xcms= that was used in the =findPeaks.matchedFilter= method for method being =lin=:
the bin size is calculated anew in each call, thus, due to rounding errors
(imprecision of floating point numbers), the bin size will be slightly different
in each call, which can lead to wrong binning results (see issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github).

Example with =profBinLinBase= resulting in an error: if =step= and =basespace= are
both =0.1= it seems that not in all buffer-generation iterations a interpolation
is initiated, i.e. the variable =ibase= in the C-function is sometimes set to =1=
(interpolation with neighboring bins) and sometimes to =0=.

This is also extensively documented in issue [[https://github.com/sneumann/xcms/issues/52][#52]].

** Different binning results due to /internal/ and /external/ breaks definition :noexport:

*FIXED*: the bin calculation in C uses now also a multiplication instead of a
addition thus resulting in identical breaks!

Breaks calculated by the =breaks_on_nBins= function are equal as breaks calculated
using the =seq= function, but they are not identical.

#+BEGIN_SRC R
  library(xcms)

  ## Define breaks from 200 to 600
  brks <- seq(200, 600, length.out = 2002)
  brks2 <- xcms:::breaks_on_nBins(200, 600, nBins = 2001)
  all.equal(brks, brks2)
  identical(brks, brks2)

  ## The difference is very small, but could still, in the binning
  ## yield slightly different results depending on which breaks are
  ## used.
  range(brks - brks2)
#+END_SRC

** Implementation and comparison for =matchedFilter=		   :noexport:

These results base on the test =dontrun_test_do_findChromPeaks_matchedFilter_impl=
defined in /test_do_findChromPeaks_matchedFilter.R/

We have 4 different functions to test and compare to the original one:
+ *A*: =.matchedFilter_orig=: it's the original code.
+ *B*: =.matchedFilter_binYonX_iter=: uses the same sequential
  buffering than the original code, but uses =binYonX= for binning and
  =imputeLinInterpol= for interpolation.
+ *C*: =.matchedFilter_no_iter=: contains the original code, but
  avoids sequential buffering, i.e. creates the whole matrix in one go.
+ *D*: =.matchedFilter_binYonX_no_iter=: my favorite: uses =binYonX= and
  =imputeLinInterpol= and avoids the sequential buffering by creating the full
  matrix in one go.

Notes: for plain =bin= we expect that results with and without iterative buffering
are identical.

*Comparisons*:
+ [X] *A* /vs/ original:
  - =bin=: always OK.
  - =binlin=: always OK.
  - =binlinbase=: always OK.
+ [X] *B* /vs/ original:
  - =bin=: OK unless =step= is =0.2=: most likely rounding problem.
  - =binlin=: only once OK. Results are not equal, but comparable.
  - =binlinbase=: similar but not equal.
+ [X] *C* /vs/ original:
  - =bin=: OK unless =step= is =0.2=:
  - =binlin=: never OK: due to interpolation on full, or subset data.
  - =binlinbase=: similar but not equal.
+ [X] *D* /vs/ original:
  - =bin=: OK unless =step= is =0.2=: most likely rounding problem.
  - =binlin=: never OK: due to interpolation on full, or subset data AND due to
    fix of the bug in =profBinLin=.
  - =binlinbase=: similar but not equal.
+ [X] *B* /vs/ *C*:
  - =bin=: always OK.
  - =binlin=: results similar but not equal; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: highly similar.
+ [X] *B* /vs/ *D*:
  - =bin=: always OK.
  - =binlin=: results similar but not equal; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: highly similar.
+ [X] *C* /vs/ *D*:
  - =bin=: always OK.
  - =binlin=: results almost identical; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: always OK.


*Conclusions*:
+ =none= (only binning, but no linear interpolation; corresponds to method =bin= in
  =findPeaks.matchedFilter=): The results are identical between all methods for
  all except one setting: with =step= being =0.2= (or =0.4= etc) on one test file the
  results differ between methods with and without iterative buffering. The
  reason for this is most likely rounding errors in floating point number
  representation: =profBin= calculates the size of the bin in each call, thus,
  when called repeatedly based on different input values, the size is slightly
  different, which then can lead to binning differences (see also [[https://github.com/sneumann/xcms/issues/47][issue #47]] on
  github).

+ =lin= (binning followed by linear interpolation to impute missing values; method
  =binlin= in =findPeaks.matchedFilter=): There are two reasons for differences
  observed here: 1) the first bin value (and eventually the last bin value) are
  sometimes wrong (issue [[https://github.com/sneumann/xcms/issues/46][#46]]). This results in differences between =binYonX= and
  =imputeKinInterpol= based approach and =profBinLin= (with the former being
  presumably correct). Also, this has a bigger influence when the
  binning/missing value imputation is performed iteratively. Thus, the
  difference between the =binYonX= - =imputeLinInterpol= and =profBinLin= approach
  without iterative buffering are only very small. 2) Linear interpolation on
  the full data set compared to subsequent sub-sets will undoubtedly lead to
  differences. Because based on the full data set, the non-iterative approach
  results in the expected and more accurate results.

+ =linbase=: results are identical if =basespace= (respectively =distance=) is such
  that no interpolation takes place. With interpolation (e.g. =distance= being =1=)
  differences (albeit small) are present between approaches with and without iterative
  buffering. The results for the approaches without iterative buffering (using
  =profBinBase= respectively =binYonX= with =imputeLinIterpol=) are identical, again
  arguing in favor of these approaches.

Thus, summarizing, the approaches without the iterative buffering yield more
reliable (and presumably correct) results. Given also that the =binYonX= in
combination with =imputeLinInterpol= identify similar peaks than the non-iterative
approaches using the original code, we can change the code to use these former
methods as default.

* Under the hood changes

These changes and updates will not have any large impact on the day-to-day use of
=xcms= and are listed here for completeness.

+ From =xcms= version 1.51.1 on the default methods from the =mzR= package are used
  for data import. Besides ensuring easier maintenance, this enables also data
  import from /gzipped/ mzML files.


* Introducing =DRanges=.						   :noexport:

*Note*: the code for this is in the =dranges= branch. The last status/problem is
that it is not quite clear how to determine the /correct/ number of decimal
places: =as.character= uses =options()$scipen= to determine how many decimal places
are represented, =sprintf= allows much more decimal places, e.g. with =%.30f=, but
these become unstable and random. The /best/ solution for now would be to limit to
a certain number of /secure/ decimal places (16?) and specify this as global
option that might be changed later. Check also =.Machine= for details on
precision, max integer etc. Note also that we are pretty much limited by the
largest =integer= that can be represented.

The =multiplier= thus has definitely be smaller than:
#+BEGIN_SRC R
  maxPos <- nchar(as.character(.Machine$integer.max))
  maxMult <- 10^maxPos

#+END_SRC

Note that we would actually just have to check that the to-be-transformed
integers don't get too large; thus we could allow more decimal places.

The idea is to use all of the =IRanges= functionality, but for any =numeric=
ranges. Examples for such ranges could be the m/z range of a feature, or the
retention time range defining a feature.

The idea is pretty simple, the =DRanges= (/D/ standing for /double/, alternatively /N/
for /numeric/) extends the =IRanges=, the =start= and =end= of the =IRanges= are
calculated by multiplying the start and end defining the numeric range by =10^d=
with =d= being the number of decimal places.

First thing is to get the number of decimal places: using code from a pretty old
post on stackoverflow
(http://stackoverflow.com/questions/5173692/how-to-return-number-of-decimal-places-in-r):


#+BEGIN_SRC R
  decimalplaces <- function(x) {
      if ((x %% 1) != 0) {
          nchar(strsplit(sub('0+$', '', as.character(x)), ".", fixed=TRUE)[[1]][[2]])
      } else {
          return(0)
      }
  }

  num.decimals <- function(x) {
      stopifnot(class(x)=="numeric")
      x <- sub("0+$","",x)
      x <- sub("^.+[.]","",x)
      nchar(x)
  }


#+END_SRC

The former is actually faster.

Eventually even =C=?
http://stackoverflow.com/questions/1083304/c-c-counting-the-number-of-decimals

#+BEGIN_EXAMPLE
  string number = "543.014";
  size_t dotFound;
  stoi(number, &dotFound));
  string(number).substr(dotFound).size()
#+END_EXAMPLE

Be aware that =number= MUST be a float/double!

alternatively:
http://stackoverflow.com/questions/9843999/calculate-number-of-decimal-places-for-a-float-value-without-libraries.

* Currently internal functionality 				   :noexport:

** =ProcessHistory=: track processing steps

This functionality comprises the =ProcessHistory= class and the =.processHistory=
slot of the =xcmsSet= objects. The =xcmsSet= function already adds a feature
detection processing step for each file to this slot. Subsetting of =xcmsSet=
objects with =[= or =split= correctly process also this slot as does concatenation
using =c=. For processing steps other than /feature detection/ a new element should
be added to the variable =.PROCSTEPS= (defined in /DataClasses.R/.
At some point we could implement methods =getProcessErrors= and =getProcessHistory=
(essentially just calling the =.getProcessErrors= and =.getProcessHistory=
functions in /functions-xcmsSet.R/.

Some additional functionality that could be implemented:
+ Sort the processing history by the =date= slot.
+ Save also analysis properties into an object extending the =ProcessHistory=:
  this would enable to get the exact settings for each processing step.

* Internal changes						   :noexport:

** Changing the way how data is imported

Random errors happen when processing a large number of files with =xcms=. This
might indicate some memory problems, eventually related to the =mzR= package
(similar to the ones spotted in =MSnbase=).

What I want to test:
+ [X] Does =mzR::openMSFile= work also for /netCDF/? No. we would have to check for
  the file type and specify the =backend= based on that.
+ [X] What about writing a new importer that does not need all the objects and
  the presumably old code in =mzR=? -> =readRawData=.

That has been fixed (see above). The /default/ methods for data import form =mzR=
are now used by default.

** Functions and methods to be deprecated and removed.

+ [ ] =xcmsSource= method: not needed anymore, reading is done by =readRawData=.
+ [ ] =loadRaw=, =initialize= for =netCdfSource= and =rampSource=: replaced by
  =readRawData=.
+ [ ] =netCdfSource= and =rampSource= S4 classes: not needed anymore, reading is
  done by =readRawData=.

** Unneeded /R/ files

+ [ ] /netCDF.R/.
+ [ ] /ramp.R/.

*** Unit tests to be removed

+ [ ] /runit.ramp.R/.

* Deprecated functions and files

Here we list all of the functions and related files that are deprecated.

+ =xcmsParallelSetup=, =xcmsPapply=, =xcmsClusterApply=: use =BiocParallel= package
  instead to setup and perform parallel processing, either /via/ the =BPPARAM=
  parameter to function and methods, or by calling =register= to globally set
  parallel processing.

+ =profBin=, =profBinM=, =profBinLin=, =profBinLinM=, =profBinLinBase=, =profBinLinBaseM=:
  replaced by the =binYonX= and =imputeLinInterpol= functions. Also, to create or
  extract the profile matrix from an =xcmsRaw= object, the =profMat= method.


** Deprecated

*** xcms 1.49:

+ =xcmsParallelSetup= (Deprecated.R)
+ =xcmsPapply= (Deprecated.R)
+ =xcmsClusterApply= (Deprecated.R)

*** xcms 1.51:

+ =profBin= (c.R)
+ =profBinM= (c.R)
+ =profBinLin= (c.R)
+ =profBinLinM= (c.R)
+ =profBinLinBase= (c.R)
+ =profBinLinBaseM= (c.R)

** Defunct

* TODOs								   :noexport:

** DONE Deprecate binning functions.
   CLOSED: [2017-02-23 Thu 07:47]

   - State "DONE"       from "TODO"       [2017-02-23 Thu 07:47]
All done except for the retention time correction!!!

** DONE Continue implementing the =do_= functions.
   CLOSED: [2017-02-23 Thu 07:47]
   - State "DONE"       from "TODO"       [2017-02-23 Thu 07:47]
** DONE Define a new object to contain the preprocessing results
   CLOSED: [2017-02-23 Thu 07:47]

   - State "DONE"       from "TODO"       [2017-02-23 Thu 07:47]
This object should replace in the long run the =xcmsSet= object providing the same
functionality while in addition add a better integration of the original raw
data files. The object should contain:

+ Peak/feature data (similar to the =xcmsSet@peaks= slot).
+ Alignment across samples information (similar to the =xcmsSet@groups= slot).
+ Corrected retention time (similar to the =xcmsSet@rt$adjusted= slot).
+ All experimental and phenotypical information.
+ A /link/ to the raw data.
+ History on data manipulation and processing.

Based on these prerequisites, an object extending Biobase's =MSnExp= or
=OnDiskMSnExp= would be ideal. The =MSnExp= would however be /too mighty/ (as it
contains all of the raw data) and the more light weight =OnDiskMSnExp= should
hence be used. While being somewhat similar to the =xcmsSet= =xcmsRaw= object setup,
the new implementation would ensure a better and less error prone import of the
raw (or even processed) data. Some data (TIC etc) are even cached within the
=OnDiskMSnExp= enabling faster data access.

Note that the lack of easy access to raw data disqualifies the =MSnSet= object
from the =MSnbase= package.

The feature data should be placed into the =assayData= environment of the object
to avoid copying etc of the data. Check also =assayDataElement()= in =MSnbase=.

*** Some notes on data usage:
+ Subset by sample: have to extract the corresponding features from the
  features matrix in =assayData= and remove all grouping/alignment
  information. This actually bypasses also the problem to check that feature
  indexes have to be updated.

+ Rename =peaks= to =features=.

+ Better alternative for =groups=: =alignedFeatures=.
+ =groupval=? =featureValues=.

*** Design and implementation:
+ =features= should be still implemented as =matrix= (for performance issues).
+ Alignment information could be implemented as =DataFrame= with the indices added
  to a column =idx=.

** DONE Rename objects, functions and methods
   CLOSED: [2017-02-23 Thu 07:47]

   - State "DONE"       from "TODO"       [2017-02-23 Thu 07:47]
+ [X] =features=: =chromPeaks=.
+ [X] =hasDetectedFeatures=: =hasChromPeaks=.
+ [ ] feature: chromatographic peak.
+ [X] =detectFeatures=: =findChromPeaks=.
+ [X] =dropFeatures=: =dropChromPeaks=.
+ [X] featureDetection-centWave: findChromPeaks-centWave
+ [X] =validFeatureMatrix=: =validChromPeaksMatrix=.

Correspondence.
+ [ ] feature groups: features (aligned and grouped chromatographic peaks).
+ [X] =groupFeatures=: =groupChromPeaks=.
+ [X] =hasAlignedFeatures=: =hasFeatures=.
+ [X] =featureGroups=: =featureDefinitions=, =featureValue= (=groupval=).
+ [X] =FeatureDensityParam=: =PeakDensityParam=.
+ [X] =NearestFeaturesParam=: =NearestPeaksParam=
+ [ ] feature alignment methods: peak alignment methods
+ [X] =$features=: =$chromPeaks=.
+ [X] =featureidx=: =peakidx=.
+ [X] =featureIndex=: =peakIndex=.
+ [X] =dropFeatureGroups=: =dropFeatureDefinitions=.
+ [ ] Peak alignment: Peak grouping
+ [X] =.PROCSTEP.PEAK.ALIGNMENT=: =.PROCSTEP.PEAK.GROUPING=.

Param classes:
+ [X] =extraFeatures=: =extraPeaks=.

RT correction.
+ [X] =featureGroups= retention time correction: =peakGroups=.
+ [X] =FeatureGroupsParam=: =PeakGroupsParam=.
+ [X] =features=: =peaks=
+ [X] =featureIndex=: =peakIndex=
+ [X] =getFeatureGroupsRtMatrix=: =getPeakGroupsRtMatrix=
+ [X] =applyRtAdjToFeatures=: =applyRtAdjToPeaks=.
+ [X] =do_groupFeatures_mzClust=: =do_groupPeaks_mzClust=.

+ [X] Check =maxFeatures= parameter for =do_groupChromPeaks_density=. Is it really
  the maximum number of features, or of peaks?

+ [X] Alignment: retention time correction between samples
  \cite{Sugimoto:2012jt}.
+ [X] Correspondence: (grouping) registration of recurring signals from the same
  analyte over replicate samples \cite{Smith:2014di}.


** DONE Implement the =Chromatogram= class
   CLOSED: [2017-07-10 Mon 15:12]

   - State "DONE"       from "TODO"       [2017-07-10 Mon 15:12]
Now, to accommodate all possibilities:
https://en.wikipedia.org/wiki/Triple_quadrupole_mass_spectrometer
Triple Q-TOF measurements:
+ Product Ion Scan
  - Q1 fixed
  - Q3 scan
+ Precursor Ion Scan
  - Q1 scan
  - Q3 fixed
+ Neutral Loss Scan
  - Q1 scan at mz = m_{product}
  - Q3 scan at mz = m_{product} - m_{neutral molecule}
+ Selected Reaction monitoring (SRM, MRM): Q1 is used to select the precursor
  ion, Q3 cycles through the product ions. Precursor/product pair is referred to
  as a /transition/.
  - Q1 fixed at mz = m_{precursor}
  - Q3 scan at mz = m_{product}


Other resources:
https://en.wikipedia.org/wiki/Mass_chromatogram#Selected-ion_monitoring_chromatogram_.28SIM.29
http://proteowizard.sourceforge.net/dox/structpwiz_1_1msdata_1_1_chromatogram.html
https://sourceforge.net/p/proteowizard/mailman/message/27571266/

*** Move =Chromatogram= to MSnbase

+ [X] Add =Chromatogram= to MSnbase.
+ [ ] Remove =Chromatogram= from xcms.
+ [ ] Move functions and methods to MSnbase.
+ [ ] Fix xcms to import all required stuff from MSnbase.


** TODO Implement a =findBackgroundIons= method

Check on one of our own files.

#+BEGIN_SRC R
  library(xcms)

  rd <- readMSData("/Volumes/Ext64/data/2016/2016-11/NoSN/250516_QC_NORM_3_POS_3.mzML", 
                   mode = "onDisk")

  ## Evaluate the mz-rt matrix - can we spot already something there?
  sps <- spectra(rd)
  dfs <- lapply(sps, as.data.frame)
  ## cut the intensities at 5000
  dfs <- lapply(dfs, function(z) {
      z[z[, "i"] > 5000, "i"] <- 5000
      return(z)
  })

  library(RColorBrewer)
  library(lattice)
  colR <- colorRampPalette(brewer.pal(9, "YlOrRd"))(255)
  brks <- do.breaks(c(0, 5000), length(colR))

  mzR <- range(mz(rd))
  rtR <- range(rtime(rd))

  plot(3, 3, pch = NA, xlim = rtR, ylim = mzR)
  for(i in 1:length(dfs)) {
      intC <- level.colors(dfs[[i]]$i, at = brks, col.regions = colR)
      xs <- rep(rtime(rd)[i], length(intC))
      points(x = xs, y = dfs[[i]]$mz, col = intC, cex = 0.1, pch = 16)
  }
  ## level.colors(x, at = brks, col.regions = colR)
#+END_SRC

A simple approach would be to walk along the mz and evaluate whether, for a
certain mz (bin?) the signal is higher than a threshold in 70% of the spectra,
i.e. that the % of values is larger than a percentage.


** DONE Reduce R CMD check time:
   CLOSED: [2017-07-10 Mon 15:12]

   - State "DONE"       from "TODO"       [2017-07-10 Mon 15:12]
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.11.3: 18m34.630s
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.9.12: 20m41.440s

After tuning xcms:
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.11.3: 14m30.454s

After enabling parallel processing for the unit tests:
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.11.3: user 21m46.385s

After enabling parallel processing (registering multicoreparam) for the unit 
tests:
- xcms 2.99.3, MSnbase 2.3.4, mzR 2.11.3: user 15m53.039s.

tests with long runtime:
+ [ ] testPresentAbsentSumAfterFillPeaks: 13.241
+ [X] test_extractChromatograms (runit.Chromatogram.R): 23.800: Can not reduce
  this.
+ [X] test_obiwarp (runit.do_adjustRtime.R): 17.594: Can not reduce this.
+ [ ] test_findChromPeaks_centWaveWithPredIsoROIs
  (runit.do_findChromPeaks_centWave_isotopes.R): 13.623
+ [X] test_do_groupChromPeaks_nearest (runit.do_groupChromPeaks.R): 25.193: OK.
+ [X] test_fillChromPeaks_matchedFilter (runit.fillChromPeaks.R): 16.843: Can
  not reduce.
+ [X] test.fillPeaks_old_vs_new (runit.fillPeaks.R): 37.924: dontrun
+ [X] test.fillPeaksColumns (runit.fillPeaks.R): 33.552: OK.
+ [X] testFillPeaksPar (runit.fillPeaks.R): 24.752: dontrun
+ [X] test_getEICxset (runit.getEIC.R): 27.144: might be faster.
+ [X] test.getEICretcor (runit.getEIC.R): 17.018: nope.
+ [X] test.issue7 (runit.getEIC.R): 66.020: dontrun
+ [X] test.getXcmsRaw (runit.getXcmsRaw.R): 26.558: might be faster.
+ [X] testMultiFactorDiffreport (runit.phenoData.R): 13.067: nothing to do.




** DONE mzR/MSnbase timings
   CLOSED: [2017-06-14 Wed 11:02]

   - State "DONE"       from "TODO"       [2017-06-14 Wed 11:02]
#+BEGIN_SRC R
  library(MSnbase)
  library(msdata)
  fl <- proteomics(full.names = TRUE)[3]


  ## MSnbase: 2.3.4
  ## mzR: 2.11.2
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.953   0.036   0.986 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.449   0.011   0.460 
  mzR::close(of)

  system.time(tmp <- readMSData(fl, mode = "onDisk"))
  ##  user  system elapsed 
  ## 1.515   0.089   1.596 

  ###########################################
  ## MSnbase: 2.3.4
  ## mzR: 2.11.3
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.974   0.039   1.009 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.422   0.010   0.433 
  mzR::close(of)

  system.time(tmp <- readMSData(fl, mode = "onDisk"))
  ##  user  system elapsed 
  ## 1.509   0.093   1.594 

  fl <- "/Users/jo/data/2016/2016-11/NoSN/190516_POOL_N_POS_14.mzML"
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.138   0.042   0.180 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.067   0.023   0.089 
  mzR::close(of)

  system.time(tmp <- readMSData(fl, mode = "onDisk"))
  ##  user  system elapsed 
  ## 0.708   0.105   0.814 

  ## tmp: 1720 spectra.

  ############################################
  ## MSnbase: 2.3.4
  ## mzR: 2.11.3, without reading the ion injection time
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.969   0.040   1.007 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.449   0.011   0.460 
  mzR::close(of)

  system.time(tmp <- readMSData(fl, mode = "onDisk"))
  ##  user  system elapsed 
  ## 1.556   0.089   1.638 

  fl <- "/Users/jo/data/2016/2016-11/NoSN/190516_POOL_N_POS_14.mzML"
  of <- mzR::openMSfile(fl, backend = "pwiz")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.138   0.064   0.214 
  mzR::close(of)

  of <- mzR::openMSfile(fl, backend = "Ramp")
  system.time(hdr <- header(of))
  ##  user  system elapsed 
  ## 0.065   0.022   0.088 
  mzR::close(of)

  system.time(tmp <- readMSData(fl, mode = "onDisk"))
  ##  user  system elapsed 
  ## 0.709   0.110   0.833 

  ## tmp: 1720 spectra.
#+END_SRC


** TODO Re-add plotting functions

There was some request to re-add the plotting functionality to back to =xcms=.
I would however like to create such plots not *during*, but *after* the
analysis. One first example would be the results from the grouping, i.e. the
=group.density= method.

+ =groupDensity=: loop through the features and create a plot for each one. The
  function could be called something like =plotGroupingResult=.
  - loop through each feature.
  - plot all peaks in the mz range of all peaks in the group (+/- something) and
    highlight the peaks belonging to the group.
  #+BEGIN_SRC R
    pks <- chromPeaks(xod)
    pks <- pks[pks[, "sample"] == 1, ]
    ## plot the rectangular data.
    xod_file <- filterFile(xod, file = 1, keepAdjustedRtime = TRUE)
    mzr <- range(mz(xod_file))
    rtr <- range(rtime(xod_file))
    plot(3, 3, pch = NA, xlim = rtr, ylim = mzr, xlab = "rentention time",
	 ylab = "mz", main = basename(fileNames(xod_file)))
    rect(xleft = pks[, "rtmin"], xright = pks[, "rtmax"], ybottom = pks[, "mzmin"],
	 ytop = pks[, "mzmax"], border = "#00000060")

    ## peak density along retention time axis.
    dens <- density(pks[, "rt"])
    plot(dens)
    hst <- hist(pks[, "rt"], breaks = 64)
    plot(hst$mids, hst$counts, type = "S")
    plot(hst)
    addi <- diff(hst$mids)[1] / 2
    points(hst$mids + addi, hst$counts, type = "S", col = "red")

    ## Plot of all peaks along retention time axis.
    hst <- lapply(split(chromPeaks(xod)[, "rt"],
			f = chromPeaks(xod)[, "sample"]),
		  hist, breaks = 64)
    max_count <- max(unlist(lapply(hst, function(z) max(z$counts))))
    ## Initialize plot:
    plot(3, 3, pch = NA, xlab = "retention time", ylab = "peak count",
	 xlim = range(rtime(xod)), ylim = c(0, max_count))
    addi <- diff(hst[[1]]$mids)[1] / 2
    lapply(hst, function(z) points(z$mids + addi, z$counts, col = "#00000060",
				   type = "S"))
  #+END_SRC

+ Plot identified chromatographic peaks. Identified and failed peaks could be
  simply plotted manually. One needs to know however where to look.
  - =plot,Chromatogram=.
  - =highlightChromPeaks=.
  - Eventually it might be nice to create a plot from above, plotting the mz vs
    rt of one file and highlighting the identified peaks: =plotChromPeaks=.
+ Plot retention time adjustment results.
  - =plotAdjustedRtime= should do the trick.
+ Plot grouping results:
  - =plot,Chromatogram=.
  - =highlightChromPeaks=.
  - =plotChromPeakDensity=.

** TODO Implement the =calibrate= method in the new user interface

First thing is to understand what the method does.
See /methods-xcmsSet.R/ for the =calibrate= method. See /matchpeaks.R/ for the
=matchpeaks= and =estimate= functions.
Input: =xcmsSet= object and list of numeric vectors representing the m/z values of
the calibrants. Apparently, the calibrants have to be close to real peaks,
otherwise they will not be adjusted/matched correctly.
For each sample:
- get the peaks of that sample, i.e. the =@peaks= matrix.
- call the =matchpeaks= function on the peaks matrix and the calibrants (which is
  supposed to be a numeric vector of mz values.

Global concept: calibration is done on the peaks. Questions:
+ Is there a global calibration value for a file we could store into the
  =XCMSnExp= object? If yes we could even apply the calibration to the individual
  mz values of a file. Actually, yes, the calibration results could be stored on
  a per-file basis in the =XCMSnExp=. Problem is we can not apply one global
  calibration to all files. So adding this to the processing queue seems to be a
  no-go.

+ We can add a function to the =processingQueue= that applies different
  adjustments depending on the =fileIdx=. Be aware! All subsetting/filtering
  approaches do have to update the file index in the =processingQueue=.

*Idea*: don't need the result class below - should be enough to add the
calibration function (inclusive parameters) to the =processingQueue= of the =MSnExp= object!

*NOTE*: to enable calibration of =mz= values of a =Spectrum=:
+ Implement a =CalibrationResult= object with slots:
  - method
  - minMz
  - maxMz
  - fileIdx
  - slope
  - intercept
+ Enable adding a =list= of these objects into =MsFeatureData=.
+ Add methods to drop/delete such objects from =MsFeatureData=.
+ =dropChromPeaks= should also drop the =list=.
+ Add function to subset the =list= in the =MsFeatureData=.
+ On subsetting: do also subset the =list=.
+ Implement a =dropCalibration= method that does restore the original mz values.



* References
